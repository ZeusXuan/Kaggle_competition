{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b307a9a4",
   "metadata": {},
   "source": [
    "从模型上来说，RoBERTa基本没有什么太大创新，主要是在BERT基础上做了几点调整： \n",
    "1）训练时间更长，batch size更大，训练数据更多； \n",
    "2）移除了next predict loss；\n",
    "3）训练序列更长； \n",
    "4）动态Masking机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf5033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import tokenizers\n",
    "import os\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8e50549",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'merges_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mhttps://huggingface.co/transformers/model_doc/roberta.html\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mhttps://huggingface.co/roberta-base#\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     MAX_LEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m      7\u001b[0m     TRAIN_BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mconfig\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m BERT_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./roberta_input\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m SAVE_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./output\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 11\u001b[0m TOKENIZER \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mByteLevelBPETokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmerges_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBERT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmerges.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlowercase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# add_prefix_space=True\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'merges_file'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "https://huggingface.co/transformers/model_doc/roberta.html\n",
    "https://huggingface.co/roberta-base#\n",
    "'''\n",
    "class config:\n",
    "    MAX_LEN = 128\n",
    "    TRAIN_BATCH_SIZE = 2\n",
    "    EPOCH = 5\n",
    "    BERT_PATH = './roberta_input'\n",
    "    SAVE_PATH = './output'\n",
    "    TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n",
    "        vocab_file=os.path.join(BERT_PATH, 'vocab.json'),\n",
    "        merges_file=os.path.join(BERT_PATH,'merges.txt'),\n",
    "        lowercase=True,\n",
    "        # add_prefix_space=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90458d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './input'\n",
    "df_train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(path, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8bfdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset:\n",
    "    def __init__(self, tweet, selected_tweet, sentiment):\n",
    "        self.tweet = tweet\n",
    "        self.selected_text = selected_tweet\n",
    "        self.sentiment = sentiment\n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "    def __getitem__(self, item):\n",
    "        tweet = self.tweet[item]\n",
    "        selected_text = self.selected_text[item]\n",
    "        sentiment = self.sentiment[item]\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "\n",
    "        tweet = ' ' + ' '.join(tweet.split())  # split()去除掉空格和\\n\n",
    "        selected_text = ' ' + ' '.join(selected_text.split())\n",
    "\n",
    "        len_st = len(selected_text) - 1\n",
    "        idx0 = None\n",
    "        idx1 = None\n",
    "\n",
    "        for i, text in enumerate(tweet):\n",
    "            if text == selected_text[1] and tweet[i:i + len_st] == selected_text[1:]:\n",
    "                idx0 = i\n",
    "                idx1 = i + len(selected_text) - 1\n",
    "                break\n",
    "        char_targets = [0] * len(tweet)\n",
    "        char_targets[idx0:idx1+1] = [1]*len(selected_text)\n",
    "\n",
    "        tok_tweet = self.tokenizer.encode(tweet)\n",
    "        input_ids_orig = tok_tweet.ids[1:-1]  # cls sep\n",
    "        tweet_offset = tok_tweet.offsets[1:-1]\n",
    "\n",
    "        target_idx = []\n",
    "        for j, (offset1, offset2) in enumerate(tweet_offset):\n",
    "            if sum(char_targets[offset1:offset2]) > 0:\n",
    "                target_idx.append(j)\n",
    "        target_start = target_idx[0]\n",
    "        target_end = target_idx[-1]\n",
    "\n",
    "        sentiment_id = {\n",
    "            'negative': 2430,\n",
    "            'neutral': 7974,\n",
    "            'positive': 1313\n",
    "        }\n",
    "        '''\n",
    "        bos_token=\"<s>\",  CLS\n",
    "        eos_token=\"</s>\", SEP\n",
    "        sep_token=\"</s>\", SEP\n",
    "        \n",
    "        - single sequence: ``<s> X </s>``\n",
    "        - pair of sequences: ``<s> A </s></s> B </s>``\n",
    "        \n",
    "        '''\n",
    "        input_ids = [0] + [sentiment_id[sentiment]] + [2,2] + input_ids_orig + [2]\n",
    "        token_type_ids = [0] * len(input_ids)\n",
    "        mask = [1] * len(input_ids)\n",
    "        tweet_offset = [(0,0)] * 4 + tweet_offset\n",
    "        target_start += 4\n",
    "        target_end += 4\n",
    "        padding_lenth = config.MAX_LEN - len(input_ids)\n",
    "        if padding_lenth > 0:\n",
    "            input_ids = input_ids + [0] * padding_lenth\n",
    "            token_type_ids = token_type_ids + [0] * padding_lenth\n",
    "            mask = mask + [0] * padding_lenth\n",
    "            tweet_offset = tweet_offset + [(0,0)] * padding_lenth\n",
    "        return {\n",
    "            'ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'tweet_off': torch.tensor(tweet_offset, dtype=torch.long),\n",
    "            'target_start': torch.tensor(target_start, dtype=torch.long),\n",
    "            'target_end': torch.tensor(target_end, dtype=torch.long),\n",
    "            'tweet': tweet,\n",
    "            'selected': selected_text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TweetDataset(tweet=df_train['text'],selected_tweet=df_train['selected_text'],sentiment=df_train['sentiment'])\n",
    "print(t.__getitem__(0))\n",
    "\n",
    "class Tweet(transformers.BertPreTrainedModel):\n",
    "    def __init__(self, conf):\n",
    "        super(Tweet, self).__init__(conf)\n",
    "        self.bert = transformers.RobertaModel.from_pretrained(config.BERT_PATH, config=conf)\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.l0 = nn.Linear(768*2, 2)\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        '''config : hidden_states = True'''\n",
    "        _, _, output = self.bert(ids, attention_mask = mask, token_type_ids=token_type_ids)\n",
    "        out = torch.cat((output[-1],output[-2]), dim=-1)\n",
    "        out = self.drop_out(out)  # 768 * 2\n",
    "        logist = self.l0(out)   # 768 * 2 -> 2\n",
    "        start_logits, end_logits = logist.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)  # [batch, length]\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a732af",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "构建loss\n",
    "'''\n",
    "def loss_fn(start_logist, end_logist, start_position, end_position):\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logist, start_position)\n",
    "    end_loss = loss_fct(end_logist, end_position)\n",
    "    return start_loss + end_loss\n",
    "\n",
    "'''\n",
    "构建optimizer\n",
    "AdamW\n",
    "'''\n",
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "model_config.output_hidden_states = True\n",
    "model = Tweet(conf=model_config)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimzer_parameter = [\n",
    "    {'params':[p for n, p in param_optimizer if not any(i in n for i in no_decay)], 'weight_decay':0.01,'lr':3e-5},\n",
    "    {'params':[p for n, p in param_optimizer if any(i in n for i in no_decay)],  'weight_decay':0.0,'lr':5e-5}\n",
    "]\n",
    "from transformers import AdamW\n",
    "optimzer = AdamW(optimzer_parameter,lr=5e-5)\n",
    "# 动态调整learning rate方式\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimzer,factor=0.1,patience=3,eps=1e-8)\n",
    "\n",
    "'''\n",
    "Early Stop\n",
    "'''\n",
    "from utils import EarlyStopping\n",
    "es = EarlyStopping(patience=3,path='./output/roberta_checkpoint.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "定义DataLoader\n",
    "'''\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(TweetDataset(df_train['text'], df_train['selected_text'], df_train['sentiment']),batch_size=config.TRAIN_BATCH_SIZE)\n",
    "\n",
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def calculate_jaccard_score(tweet,orig_selected, start_logist, end_logist, offset):\n",
    "    if start_logist > end_logist:\n",
    "        start_logist = end_logist\n",
    "    # offset (0,1), (1,9)\n",
    "    logist_selected = tweet[offset[start_logist][0] : offset[end_logist][1]]\n",
    "    return jaccard(orig_selected, logist_selected)\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "def main():\n",
    "    for i in range(config.EPOCH):\n",
    "        tk0 = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "        losses = 0\n",
    "        for i, data in enumerate(tk0):\n",
    "            start_logist, end_logist = model(data['ids'], data['mask'], data['token_type_ids'])\n",
    "            loss = loss_fn(start_logist, end_logist, data['target_start'], data['target_end'])\n",
    "            losses += loss*len(data['ids'])\n",
    "            optimzer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimzer.step()\n",
    "            output_start = torch.argmax(start_logist, dim=-1)\n",
    "            output_end = torch.argmax(end_logist, dim=-1)\n",
    "            jaccards = []\n",
    "            for p_i, tweet in enumerate(data['tweet']):\n",
    "                jaccard_s = calculate_jaccard_score(tweet, data['selected'][p_i], output_start[p_i], output_end[p_i], data['tweet_off'][p_i])\n",
    "                jaccards.append(jaccard_s)\n",
    "            tk0.set_postfix({'loss':loss.item(),'jaccard':np.mean(jaccards)})\n",
    "        scheduler.step(losses)\n",
    "        es(losses, model)\n",
    "        if es.early_stop:\n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
