{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b4b93c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import tokenizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55c40f76",
   "metadata": {},
   "source": [
    "分析数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ce1cebf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   textID         27481 non-null  object\n",
      " 1   text           27480 non-null  object\n",
      " 2   selected_text  27480 non-null  object\n",
      " 3   sentiment      27481 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 858.9+ KB\n",
      "None\n",
      "       textID                                               text  \\\n",
      "0  cb774db0d1                I`d have responded, if I were going   \n",
      "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
      "2  088c60f138                          my boss is bullying me...   \n",
      "3  9642c003ef                     what interview! leave me alone   \n",
      "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
      "\n",
      "                         selected_text sentiment  \n",
      "0  I`d have responded, if I were going   neutral  \n",
      "1                             Sooo SAD  negative  \n",
      "2                          bullying me  negative  \n",
      "3                       leave me alone  negative  \n",
      "4                        Sons of ****,  negative  \n"
     ]
    }
   ],
   "source": [
    "# 将数据显示完全\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "path = './input'\n",
    "df_train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
    "print(df_train.info())\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c47e2bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 空值非常少, 直接删除带有空值的项\n",
    "df_train.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02742abd",
   "metadata": {},
   "source": [
    "构建DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2f3f9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义全局参数\n",
    "# 模型见: https://huggingface.co/model  bert_base_uncased\n",
    "class config:\n",
    "    MAX_LENTH = 128\n",
    "    TRAIN_BATCH_SIZE = 2\n",
    "    TEST_BATCH_SIZE = 32\n",
    "    EPOCH = 3\n",
    "    BERT_PATH = './input'\n",
    "    MODEL_PATH = 'pytorch_model.bin'\n",
    "    TOKENIZER = tokenizers.BertWordPieceTokenizer(os.path.join(BERT_PATH, 'vocab.txt'), lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3c3088da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 4997, 8699, 3893, 102]\n",
      "['[CLS]', 'negative', 'neutral', 'positive', '[SEP]']\n",
      "[(0, 0), (0, 8), (9, 16), (17, 25), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "BERT的tokenizer输出\n",
    "1. ids:对应token在vocab中的id, <cls>为101,<seq>为102\n",
    "2. type_ids\n",
    "3. tokens\n",
    "4. offsets\n",
    "'''\n",
    "tmp = config.TOKENIZER.encode('negative neutral positive')\n",
    "print(tmp.ids)\n",
    "print(tmp.tokens)\n",
    "print(tmp.offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "668dab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset:\n",
    "    def __init__(self, tweet, selected_tweet, sentiment):\n",
    "        self.tweet = tweet\n",
    "        self.selected_text = selected_tweet\n",
    "        self.sentiment = sentiment\n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "    def __getitem__(self, item):\n",
    "        tweet = self.tweet[item]\n",
    "        selected_text = self.selected_text[item]\n",
    "        sentiment = self.sentiment[item]\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "        '''\n",
    "        BERT模型需要的输入格式\n",
    "        1) ids:text -> index(input_ids)\n",
    "        2) mask: 参与到self-attention\n",
    "        3) token_type_ids: 标识两个句子,用于NSP\n",
    "        '''\n",
    "        # 1）找到训练的标签：start，end\n",
    "        idx0 = None\n",
    "        idx1 = None\n",
    "        for i, text in enumerate(tweet):\n",
    "            if text == selected_text[0] and tweet[i:i + len(selected_text)] == selected_text:\n",
    "                idx0 = i\n",
    "                idx1 = i + len(selected_text) - 1\n",
    "        # 对tweet进行tokenizer\n",
    "        tok_tweet = self.tokenizer.encode(tweet)\n",
    "        input_ids_orig = tok_tweet.ids[1:-1]\n",
    "        tweet_offset = tok_tweet.offsets[1:-1]\n",
    "        \n",
    "        char_target = [0] * len(tweet)\n",
    "        char_target[idx0:idx1+1] = [1] * len(selected_text)\n",
    "\n",
    "        target_idx = []\n",
    "        for j, (offset1, offset2) in enumerate(tweet_offset):\n",
    "            if sum(char_target[offset1:offset2]) > 0:\n",
    "                target_idx.append(j)\n",
    "        target_start = target_idx[0]\n",
    "        target_end = target_idx[-1]\n",
    "        \n",
    "        '''\n",
    "        ids, mask, token_type_ids\n",
    "        '''\n",
    "        sentiment_id = {\n",
    "            'negative':4997,\n",
    "            'neutral':8699,\n",
    "            'positive':3893\n",
    "        }\n",
    "        # 格式为 <CLS>,sentiment,<SEP>,tweet,<SEP>\n",
    "        input_ids = [101] + [sentiment_id[sentiment]] + [102] + input_ids_orig\n",
    "        # 句子分界\n",
    "        token_type_ids = [0, 0, 0] + [1] * (len(input_ids) - 3)\n",
    "        mask = [1] * len(input_ids)\n",
    "        tweet_offset = [(0,0)] * 3 + tweet_offset\n",
    "        target_start += 3\n",
    "        target_end += 3\n",
    "        \n",
    "        # padding, max_len < 128\n",
    "        padding_lenght = config.MAX_LENTH - len(input_ids)\n",
    "        if padding_lenght > 0:\n",
    "            input_ids = input_ids + [0] * padding_lenght\n",
    "            token_type_ids = token_type_ids + [0] * padding_lenght\n",
    "            mask = mask + [0] * padding_lenght\n",
    "            tweet_offset = tweet_offset + ([(0,0)] * (padding_lenght))\n",
    "        \n",
    "        return {\n",
    "            'ids':torch.tensor(input_ids, dtype=torch.long),\n",
    "            'token_type_ids':torch.tensor(token_type_ids,dtype=torch.long),\n",
    "            'mask':torch.tensor(mask,dtype=torch.long),\n",
    "            'tweet_off':torch.tensor(tweet_offset,dtype=torch.long),\n",
    "            'target_start':torch.tensor(target_start, dtype=torch.long),\n",
    "            'target_end':torch.tensor(target_end, dtype=torch.long),\n",
    "            'tweet':tweet,\n",
    "            'selected':selected_text\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "173b23d9",
   "metadata": {},
   "source": [
    "定义模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4d3df5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BERT的输出：\n",
    "https://www.cnblogs.com/deep-deep-learning/p/12792041.html\n",
    "sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "1) sequence_output：输出的序列所有单词的embedding [batch, length, embedding]\n",
    "2) pooled_output: CLS的输出[batch, embedding]\n",
    "3) hidden_states: 输出BERT模型所有层的输出(13层的transformer block) 13 * [batch, length, embedding](model_hidden_states=True)\n",
    "4) attenions: 输出attentions\n",
    "'''\n",
    "class Tweet(transformers.BertPreTrainedModel):\n",
    "    def __init__(self,conf):\n",
    "        super(Tweet, self).__init__(conf)\n",
    "        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH,config=conf)\n",
    "        \n",
    "        # 这里选择了调整整个模型参数而非全连接层\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        # 设置全连接层, 分别输出start和end的位置\n",
    "        self.l0 = nn.Linear(768*2, 2)\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        '''config : hidden_states = True'''\n",
    "        _, _, output = self.bert(ids, attention_mask = mask, token_type_ids=token_type_ids)\n",
    "        # 合并output的最后一层和倒数第二层的输出\n",
    "        out = torch.cat((torch.tensor(output[-1]),torch.tensor(output[-2])), dim=-1)\n",
    "        out = self.drop_out(out)  # 768 * 2\n",
    "        logist = self.l0(out)   # 768 * 2 -> 2\n",
    "        start_logits, end_logits = logist.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)  # [batch, length]\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cac2b0d5",
   "metadata": {},
   "source": [
    "定义Optimize和Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "eccab953",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "构建loss,交叉熵\n",
    "'''\n",
    "def loss_fn(start_logist, end_logist, start_position, end_position):\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logist, start_position)\n",
    "    end_loss = loss_fct(end_logist, end_position)\n",
    "    return start_loss + end_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "37757d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./input were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "构建optimizer\n",
    "AdamW\n",
    "'''\n",
    "# 构建模型\n",
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "model_config.output_hidden_states = True\n",
    "model = Tweet(conf=model_config)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "\n",
    "# 对LayerNorm层不设置正则项且学习率更小\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimzer_parameter = [\n",
    "    {'params':[p for n, p in param_optimizer if not any(i in n for i in no_decay)], 'weight_decay':0.01,'lr':3e-5},\n",
    "    {'params':[p for n, p in param_optimizer if any(i in n for i in no_decay)],  'weight_decay':0.0,'lr':5e-5}\n",
    "]\n",
    "from transformers import AdamW\n",
    "optimzer = AdamW(optimzer_parameter,lr=5e-5)\n",
    "\n",
    "# 动态调整learning rate方式\n",
    "'''\n",
    "optimizer (Optimizer) – Wrapped optimizer.\n",
    "\n",
    "factor (float) – Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: 0.1.\n",
    "\n",
    "patience (int) – Number of epochs with no improvement after which learning rate will be reduced. \n",
    "For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, \n",
    "and will only decrease the LR after the 3rd epoch if the loss still hasn’t improved then. Default: 10.\n",
    "\n",
    "threshold (float) – Threshold for measuring the new optimum, to only focus on significant changes. Default: 1e-4.\n",
    "'''\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimzer,factor=0.1,patience=3,eps=1e-8)\n",
    "\n",
    "'''\n",
    "Early Stop,避免可能的灾难性遗忘\n",
    "'''\n",
    "from utils import EarlyStopping\n",
    "es = EarlyStopping(patience=3,path='./output/checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f3501818",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "定义DataLoader\n",
    "'''\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(TweetDataset(df_train['text'], df_train['selected_text'], df_train['sentiment']),batch_size=config.TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d631a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "该任务的评价指标为字符级别的Jaccard相似度\n",
    "'''\n",
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def calculate_jaccard_score(tweet,orig_selected, start_logist, end_logist, offset):\n",
    "    if start_logist > end_logist:\n",
    "        start_logist = end_logist\n",
    "    # offset (0,1), (1,9)\n",
    "    logist_selected = tweet[offset[start_logist][0] : offset[end_logist][1]]\n",
    "    return jaccard(orig_selected, logist_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8c4773b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "def main():\n",
    "    for i in range(config.EPOCH):\n",
    "        tk0 = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "        losses = 0\n",
    "        for i, data in enumerate(tk0):\n",
    "            start_logist, end_logist = model(data['ids'], data['mask'], data['token_type_ids'])\n",
    "            loss = loss_fn(start_logist, end_logist, data['target_start'], data['target_end'])\n",
    "            losses += loss*len(data['ids'])\n",
    "            optimzer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimzer.step()\n",
    "            output_start = torch.argmax(start_logist, dim=-1)\n",
    "            output_end = torch.argmax(end_logist, dim=-1)\n",
    "            jaccards = []\n",
    "            for p_i, tweet in enumerate(data['tweet']):\n",
    "                jaccard_s = calculate_jaccard_score(tweet, data['selected'][p_i], output_start[p_i], output_end[p_i], data['tweet_off'][p_i])\n",
    "                jaccards.append(jaccard_s)\n",
    "            tk0.set_postfix({'loss':loss.item(),'jaccard':np.mean(jaccards)})\n",
    "        scheduler.step(losses)\n",
    "        es(losses, model)\n",
    "        if es.early_stop:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dbeac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
